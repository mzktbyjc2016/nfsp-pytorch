import LeducPoker.LeducPokerGame as LeducPoker
import LeducPoker.Policies as Policies
from LeducPoker.LeducPokerGame import PlayerActions
import numpy as np
from typing import Tuple, Iterable, List, Mapping
import copy
import itertools


NUM_BOARD = 3
NUM_HOLDINGS = 3


class HeinrichExploitability(object):
    def extract_probs(self, state: LeducPoker.LeducGameState, policy: Policies.Policy, num_board_cards):
        probs = np.zeros((num_board_cards, NUM_HOLDINGS, 3))
        infoset: LeducPoker.LeducInfoset = copy.deepcopy(state.infosets[state.player_to_act])

        for b in range(num_board_cards):
            if num_board_cards > 1:
                infoset.board_card = b
            else:
                infoset.board_card = None
            for h in range(NUM_HOLDINGS):
                infoset.card = h
                probs[b][h] = policy.action_prob(infoset)

        return probs

    def extract_terminal_values(self, state: LeducPoker.LeducGameState, reach: np.ndarray, values: np.ndarray):
        values.fill(0)
        no_board = state.game_round == 0

        for b in range(values.shape[1]):
            for h1 in range(NUM_HOLDINGS):
                if h1 == b:
                    board_collisions = 1
                else:
                    board_collisions = 0
                for h2 in range(NUM_HOLDINGS):
                    if no_board:
                        board_prob = 1
                    else:
                        if h2 == b:
                            board_collisions += 1
                        # compute board prob
                        if board_collisions == 2:
                            board_prob = 0
                        elif board_collisions == 1:
                            board_prob = 0.25
                        else:
                            board_prob = 0.5
                    # Compute holding prob
                    if h1 == h2:
                        holding_prob = 0.2 / 3
                    else:
                        holding_prob = 0.4 / 3
                    # get absolute terminal rewards
                    state.board_card = b
                    state.player_cards[0] = h1
                    state.player_cards[1] = h2
                    state._update_infosets()

                    rewards = state.get_payoffs()
                    rewards -= rewards.sum() / 2  # Equivalent to subtracting off the cumulative

                    # accumulate values
                    values[0][b][h1] += reach[1][b][h2] * holding_prob * board_prob * rewards[0]
                    values[1][b][h2] += reach[0][b][h1] * holding_prob * board_prob * rewards[1]

                    if h2 == b:
                        board_collisions -= 1

    def _eval(
            self,
            policies: Policies.Policy,
            state: LeducPoker.LeducGameState,
            reach: np.ndarray,
            prev_values: np.ndarray):
        if state.is_terminal:
            self.extract_terminal_values(state, reach, prev_values)
        else:
            player = state.player_to_act
            opponent = (player + 1) % 2
            num_board_cards = 1 if (state.game_round == 0) else NUM_BOARD

            max_player_values = np.ones((num_board_cards, NUM_HOLDINGS)) * -999
            max_player_actions = np.zeros((num_board_cards, NUM_HOLDINGS))
            opponent_values = np.zeros((num_board_cards, NUM_HOLDINGS))

            values = np.zeros((2, num_board_cards, NUM_HOLDINGS))

            probs = self.extract_probs(state, policies, num_board_cards)

            for a in PlayerActions.ALL_ACTIONS:
                if state.can_take_action(a):
                    next_state = copy.deepcopy(state)
                    next_state.add_action(a)
                    if next_state.player_to_act == -1:
                        next_state.board_card = 0
                    next_reach = reach.copy()
                    for b in range(NUM_BOARD):
                        eff_b = b % num_board_cards
                        for h in range(NUM_HOLDINGS):
                            next_reach[player][b][h] *= probs[eff_b][h][a]

                    self._eval(policies, next_state, next_reach, values)
                    # Evaluate values
                    for b in range(num_board_cards):
                        for h in range(NUM_HOLDINGS):
                            # pick highest values/actions for player
                            if max_player_values[b][h] < values[player][b][h]:
                                max_player_values[b][h] = values[player][b][h]
                                max_player_actions[b][h] = a
                            # sum values for opponent (as already prob weighted)
                            opponent_values[b][h] += values[opponent][b][h]

            # update prev values
            prev_values.fill(0)
            num_prev_board_cards = prev_values.shape[1]
            for b in range(num_board_cards):
                eff_b = b % num_prev_board_cards
                for h in range(NUM_HOLDINGS):
                    prev_values[player][eff_b][h] += max_player_values[b][h]
                    prev_values[opponent][eff_b][h] += opponent_values[b][h]

    def eval_exploitability(self, policy: Policies.Policy):
        values = np.zeros((2, 1, NUM_HOLDINGS))
        reach = np.ones((2, NUM_BOARD, NUM_HOLDINGS))

        self._eval(policy, LeducPoker.LeducGameState(
            player_cards=[0, 1], bet_sequences=[(), ()], board_card=None), reach, values)

        exploitability = np.zeros(2)

        for p in range(2):
            for h in range(NUM_HOLDINGS):
                exploitability[p] += values[p][0][h]

        return exploitability


if __name__ == "__main__":
    nash_policy = Policies.NashPolicy(
        p0_strat_filename="/home/tjohnson/PycharmProjects/PokerRL/LeducPoker/fullgame_strats/strat1",
        p1_strat_filename="/home/tjohnson/PycharmProjects/PokerRL/LeducPoker/fullgame_strats/strat2"
    )
    _heinrich_exploitability = HeinrichExploitability()

    import time
    _start = time.time()
    _exploitability = _heinrich_exploitability.eval_exploitability(nash_policy)
    _end = time.time()
    print("Calc time: ", _end - _start)
    print("Nash: %s (Sum %s)" % (_exploitability, _exploitability.sum()))  # Game values should be +- 0.0858

    nash_policy = Policies.NashPolicy(
        p0_strat_filename="/home/tjohnson/PycharmProjects/PokerRL/LeducPoker/fullgame_strats/partial1",
        p1_strat_filename="/home/tjohnson/PycharmProjects/PokerRL/LeducPoker/fullgame_strats/partial2"
    )
    import time
    _start = time.time()
    _exploitability = _heinrich_exploitability.eval_exploitability(nash_policy)
    _end = time.time()
    print("Calc time: ", _end - _start)
    print("Exploitability (expected 1.92, 2.61): %s sum: %s" % (_exploitability, sum(_exploitability)))  # Game values should be 1.92495, 2.61482
    #
